{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For raw data, get l1-name and l1-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_l1(cate_idx):\n",
    "    l2_index = int(cate_idx.split('.')[0])\n",
    "    if 1 <= l2_index <= 2:\n",
    "        return \"System and Operational Risks\", 1\n",
    "    elif 3 <= l2_index <= 7:\n",
    "        return \"Content Safety Risks\", 2\n",
    "    elif 8 <= l2_index <= 12:\n",
    "        return \"Societal Risks\", 3\n",
    "    elif 13 <= l2_index <= 16:\n",
    "        return \"Legal and Rights-Related Risks\", 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai_gpt-3.5-turbo-0125_result.csv\n",
      "column l1-name already exist\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"./raw_data\"\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(filename)\n",
    "        infile_path = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(infile_path)\n",
    "        \n",
    "        if \"l1-name\" in df.columns:\n",
    "            print(\"column l1-name already exist\")\n",
    "            break\n",
    "        \n",
    "        l1_name_list = []\n",
    "        new_cate_idx_list = []\n",
    "        for _, row in df.iterrows():\n",
    "            cate_idx = row['cate-idx']\n",
    "            l1_name, l1_index = get_l1(cate_idx)\n",
    "            \n",
    "            l1_name_list.append(l1_name)\n",
    "            new_cate_idx_list.append(f\"{l1_index}.{cate_idx}\")\n",
    "\n",
    "        df.insert(1, 'l1-name', l1_name_list)\n",
    "        df['cate-idx'] = new_cate_idx_list\n",
    "        df.to_csv(infile_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_name(model):\n",
    "    if model == '01-ai_yi-34b-chat':\n",
    "        return 'Yi Chat (34B)'\n",
    "    elif model == 'anthropic_claude-3-haiku-20240307':\n",
    "        return 'Claude 3 Haiku'\n",
    "    elif model == 'anthropic_claude-3-opus-20240229':\n",
    "        return 'Claude 3 Opus'\n",
    "    elif model == 'anthropic_claude-3-sonnet-20240229':\n",
    "        return 'Claude 3 Sonnet'\n",
    "    elif model == 'cohere_command-r-plus':\n",
    "        return 'Cohere Command R Plus'\n",
    "    elif model == 'cohere_command-r':\n",
    "        return 'Cohere Command R'\n",
    "    elif model == 'databricks_dbrx-instruct':\n",
    "        return 'DBRX Instruct'\n",
    "    elif model == 'deepseek-ai_deepseek-llm-67b-chat':\n",
    "        return 'DeepSeek LLM Chat (67B)'\n",
    "    elif model == 'google_gemini-1.5-flash-001-safety-default':\n",
    "        return 'Gemini 1.5 Flash'\n",
    "    elif model == 'google_gemini-1.5-pro-001-safety-default':\n",
    "        return 'Gemini 1.5 Pro'\n",
    "    elif model == 'meta_llama-3-8b-chat':\n",
    "        return 'Llama 3 Chat (8B)'\n",
    "    elif model == 'meta_llama-3-70b-chat':\n",
    "        return 'Llama 3 Chat (70B)'\n",
    "    elif model == 'mistralai_mistral-7b-instruct-v0.3':\n",
    "        return 'Mistral Instruct v0.3 (7B)'\n",
    "    elif model == 'mistralai_mixtral-8x7b-instruct-v0.1':\n",
    "        return 'Mixtral Instruct (8x7B)'\n",
    "    elif model == 'mistralai_mixtral-8x22b-instruct-v0.1':\n",
    "        return 'Mixtral Instruct (8x22B)'\n",
    "    elif model == 'openai_gpt-3.5-turbo-0613':\n",
    "        return 'GPT-3.5 Turbo (0613)'\n",
    "    elif model == 'openai_gpt-3.5-turbo-0125':\n",
    "        return 'GPT-3.5 Turbo (0125)'\n",
    "    elif model == 'openai_gpt-3.5-turbo-1106':\n",
    "        return 'GPT-3.5 Turbo (1106)'\n",
    "    elif model == 'openai_gpt-4-turbo-2024-04-09':\n",
    "        return 'GPT-4 Turbo'\n",
    "    elif model == 'openai_gpt-4o-2024-05-13':\n",
    "        return 'GPT-4o'\n",
    "    elif model == 'qwen_qwen1.5-72b-chat':\n",
    "        return 'Qwen1.5 Chat (72B)'\n",
    "    else:\n",
    "        return 'Unknown model'\n",
    "    \n",
    "def count_non_none(row):\n",
    "    count = sum(1 for cell in row if cell != 'None')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3.5 Turbo (0125)\n",
      "Llama 3 Chat (8B)\n",
      "Yi Chat (34B)\n",
      "Cohere Command R\n",
      "Qwen1.5 Chat (72B)\n",
      "GPT-4o\n",
      "DBRX Instruct\n",
      "Mixtral Instruct (8x22B)\n",
      "Cohere Command R Plus\n",
      "Claude 3 Haiku\n",
      "Claude 3 Opus\n",
      "Gemini 1.5 Flash\n",
      "GPT-3.5 Turbo (0613)\n",
      "Llama 3 Chat (70B)\n",
      "Gemini 1.5 Pro\n",
      "GPT-3.5 Turbo (1106)\n",
      "GPT-4 Turbo\n",
      "Mixtral Instruct (8x7B)\n",
      "Claude 3 Sonnet\n",
      "Mistral Instruct v0.3 (7B)\n",
      "DeepSeek LLM Chat (67B)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_dir = \"./raw_data\"\n",
    "outfile_path = \"./merged_data.csv\"\n",
    "\n",
    "merged_df = pd.DataFrame()\n",
    "# get column 1-6\n",
    "first_file = pd.read_csv(\"./raw_data/01-ai_yi-34b-chat_result.csv\")\n",
    "merged_df = first_file.iloc[:, :6].copy()\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        infile_path = os.path.join(input_dir, filename)\n",
    "        infile = pd.read_csv(infile_path)\n",
    "\n",
    "        model = filename.split(\"_result.csv\")[0]\n",
    "        model = format_model_name(model)\n",
    "        print(model)\n",
    "\n",
    "        response_col = infile['response']\n",
    "        score_col = infile['score']\n",
    "\n",
    "        # if score = 0.5/0, replace response with None\n",
    "        response_col = response_col.where((score_col == 1), 'None')\n",
    "        merged_df[model] = response_col\n",
    "\n",
    "merged_df['total_response'] = merged_df.iloc[:, 6:27].apply(count_non_none, axis=1)\n",
    "\n",
    "merged_df.to_csv(outfile_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

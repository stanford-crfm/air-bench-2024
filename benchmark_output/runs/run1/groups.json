[
  {
    "title": "All scenarios",
    "header": [
      {
        "value": "Group",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Description",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Adaptation method",
        "description": "Adaptation strategy (e.g., generation)",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# instances",
        "description": "Number of instances evaluated on",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# references",
        "description": "Number of references provided per instance",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# prompt tokens",
        "description": "Total number of prompt tokens",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# completion tokens",
        "description": "Total number of completion tokens",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# models",
        "description": "Number of models we're evaluating",
        "markdown": false,
        "metadata": {}
      }
    ],
    "rows": [
      [
        {
          "value": "Core scenarios",
          "href": "?group=core_scenarios",
          "markdown": false
        },
        {
          "value": "The scenarios where we evaluate all the models.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Targeted evaluations",
          "href": "?group=targeted_evaluations",
          "markdown": false
        },
        {
          "value": "Targeted evaluation of specific skills (e.g., knowledge, reasoning) and risks (e.g., disinformation, memorization/copyright).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ]
    ],
    "links": []
  },
  {
    "title": "Core scenarios",
    "header": [
      {
        "value": "Group",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Description",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Adaptation method",
        "description": "Adaptation strategy (e.g., generation)",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# instances",
        "description": "Number of instances evaluated on",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# references",
        "description": "Number of references provided per instance",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# prompt tokens",
        "description": "Total number of prompt tokens",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# completion tokens",
        "description": "Total number of completion tokens",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# models",
        "description": "Number of models we're evaluating",
        "markdown": false,
        "metadata": {}
      }
    ],
    "rows": [
      [
        {
          "value": "Question answering",
          "href": "?group=question_answering",
          "markdown": false
        },
        {
          "value": "In question answering, given a question and (optionally, in open-book settings) a passage, the goal is to produce the answer. QA is a general format that captures a wide range of tasks involving varying levels of world and commonsense knowledge and reasoning abilities.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Information retrieval",
          "href": "?group=information_retrieval",
          "markdown": false
        },
        {
          "value": "In information retrieval, given a query and a set of candidate documents, the goal is to produce a ranking of the documents.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Summarization",
          "href": "?group=summarization",
          "markdown": false
        },
        {
          "value": "In text summarization, given a piece of text (paragraph or document), the goal is to produce a much shorter summary.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Sentiment analysis",
          "href": "?group=sentiment_analysis",
          "markdown": false
        },
        {
          "value": "In sentiment classification, given a text (e.g., movie review), the goal is to predict the sentiment (positive or negative).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Toxicity detection",
          "href": "?group=toxicity_detection",
          "markdown": false
        },
        {
          "value": "In toxicity detection, given a text, the goal is to predict whether the text has toxic content.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Text classification",
          "href": "?group=miscellaneous_text_classification",
          "markdown": false
        },
        {
          "value": "Text classification is a general format that aims to classify text into a set of categories. This includes a wide range of classification tasks where the input is text.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Aspirational scenarios",
          "href": "?group=aspirational",
          "markdown": false
        },
        {
          "value": "Scenarios that we should support.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "DecodingTrust",
          "href": "?group=decodingtrust",
          "markdown": false
        },
        {
          "value": "A comprehensive benchmark of the trustworthiness of large language models [(Wang et. al. 2023)](https://decodingtrust.github.io/)",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ]
    ],
    "links": []
  },
  {
    "title": "Targeted evaluations",
    "header": [
      {
        "value": "Group",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Description",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Adaptation method",
        "description": "Adaptation strategy (e.g., generation)",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# instances",
        "description": "Number of instances evaluated on",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# references",
        "description": "Number of references provided per instance",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# prompt tokens",
        "description": "Total number of prompt tokens",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# completion tokens",
        "description": "Total number of completion tokens",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# models",
        "description": "Number of models we're evaluating",
        "markdown": false,
        "metadata": {}
      }
    ],
    "rows": [
      [
        {
          "value": "Language",
          "href": "?group=language",
          "markdown": false
        },
        {
          "value": "Targeted evaluation of linguistic capabilities.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Knowledge",
          "href": "?group=knowledge",
          "markdown": false
        },
        {
          "value": "Targeted evaluation of knowledge (e.g. factual, cultural, commonsense).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Reasoning",
          "href": "?group=reasoning",
          "markdown": false
        },
        {
          "value": "Targeted evaluation of reasoning capabilities (e.g. mathematical, hierarchical).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Harms",
          "href": "?group=harms",
          "markdown": false
        },
        {
          "value": "Targeted evaluation of social harms (e.g., copyright, disinformation, social bias, toxicity).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Efficiency",
          "href": "?group=efficiency",
          "markdown": false
        },
        {
          "value": "Targeted evaluation of training and inference efficiency.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Calibration",
          "href": "?group=calibration",
          "markdown": false
        },
        {
          "value": "Extended calibration metrics.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Vary number of in-context examples",
          "href": "?group=ablation_in_context",
          "markdown": false
        },
        {
          "value": "Vary the number of in-context training examples.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Vary multiple-choice strategy",
          "href": "?group=ablation_multiple_choice",
          "markdown": false
        },
        {
          "value": "Vary the adapation strategy for multiple-choice questions.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Vary prompting",
          "href": "?group=ablation_prompts",
          "markdown": false
        },
        {
          "value": "Vary the instructions and labels for input/output.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Robustness to contrast sets",
          "href": "?group=robustness_contrast_sets",
          "markdown": false
        },
        {
          "value": "Evaluating equivariance to semantics-altering perturbations",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Robustness to single types of perturbations",
          "href": "?group=robustness_individual",
          "markdown": false
        },
        {
          "value": "Evaluating robsustness to a single perturbation at a time (e.g., typos, synonyms)",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) scenarios",
          "href": "?group=chinese_cleva",
          "markdown": false
        },
        {
          "value": "Scenarios for evaluating Chinese language models",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ]
    ],
    "links": []
  },
  {
    "title": "Scenarios",
    "header": [
      {
        "value": "Group",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Description",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Adaptation method",
        "description": "Adaptation strategy (e.g., generation)",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# instances",
        "description": "Number of instances evaluated on",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# references",
        "description": "Number of references provided per instance",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# prompt tokens",
        "description": "Total number of prompt tokens",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# completion tokens",
        "description": "Total number of completion tokens",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "# models",
        "description": "Number of models we're evaluating",
        "markdown": false,
        "metadata": {}
      }
    ],
    "rows": [
      [
        {
          "value": "BoolQ",
          "href": "?group=boolq",
          "markdown": false
        },
        {
          "value": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "NarrativeQA",
          "href": "?group=narrative_qa",
          "markdown": false
        },
        {
          "value": "The NarrativeQA benchmark for reading comprehension over narratives [(Ko\u010disk\u00fd et al., 2017)](https://aclanthology.org/Q18-1023/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "NaturalQuestions (closed-book)",
          "href": "?group=natural_qa_closedbook",
          "markdown": false
        },
        {
          "value": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "NaturalQuestions (open-book)",
          "href": "?group=natural_qa_openbook_longans",
          "markdown": false
        },
        {
          "value": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "QuAC (Question Answering in Context)",
          "href": "?group=quac",
          "markdown": false
        },
        {
          "value": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "HellaSwag",
          "href": "?group=hellaswag",
          "markdown": false
        },
        {
          "value": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "OpenbookQA",
          "href": "?group=openbookqa",
          "markdown": false
        },
        {
          "value": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "TruthfulQA",
          "href": "?group=truthful_qa",
          "markdown": false
        },
        {
          "value": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "MMLU (Massive Multitask Language Understanding)",
          "href": "?group=mmlu",
          "markdown": false
        },
        {
          "value": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "MS MARCO (regular track)",
          "href": "?group=msmarco_regular",
          "markdown": false
        },
        {
          "value": "The MS MARCO benchmark's regular track for passage retrieval in information retrieval [(https://microsoft.github.io/msmarco/)](https://microsoft.github.io/msmarco/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "MS MARCO (TREC track)",
          "href": "?group=msmarco_trec",
          "markdown": false
        },
        {
          "value": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CNN/DailyMail",
          "href": "?group=summarization_cnndm",
          "markdown": false
        },
        {
          "value": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "XSUM",
          "href": "?group=summarization_xsum",
          "markdown": false
        },
        {
          "value": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "IMDB",
          "href": "?group=imdb",
          "markdown": false
        },
        {
          "value": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "RAFT (Real-world Annotated Few-Shot)",
          "href": "?group=raft",
          "markdown": false
        },
        {
          "value": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CivilComments",
          "href": "?group=civil_comments",
          "markdown": false
        },
        {
          "value": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "ICE (International Corpus of English)",
          "href": "?group=ice",
          "markdown": false
        },
        {
          "value": "The International Corpus of English (ICE) drawn from English speakers from various places in the world, initiated by [Greenbaum (1991)](https://www.cambridge.org/core/journals/english-today/article/abs/ice-the-international-corpus-of-english/47808205394C538393C3FD8E62E5E701).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "The Pile",
          "href": "?group=the_pile",
          "markdown": false
        },
        {
          "value": "The Pile corpus for measuring lanugage model performance across various domains [(Gao et al., 2020)](https://arxiv.org/pdf/2101.00027.pdf).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "TwitterAAE",
          "href": "?group=twitter_aae",
          "markdown": false
        },
        {
          "value": "The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance in tweets as a function of speaker dialect.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "TwitterAAE (AA)",
          "href": "?group=twitter_aae_aa",
          "markdown": false
        },
        {
          "value": "The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance on African-American-aligned Tweets.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "TwitterAAE (white)",
          "href": "?group=twitter_aae_white",
          "markdown": false
        },
        {
          "value": "The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance on White-aligned Tweets.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "BLiMP (The Benchmark of Linguistic Minimal Pairs for English)",
          "href": "?group=blimp",
          "markdown": false
        },
        {
          "value": "The Benchmark of Linguistic Minimal Pairs for English (BLiMP) for measuring performance on linguistic phenomena using minimal pair design [(Warstadt et al., 2020)](https://aclanthology.org/2020.tacl-1.25/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "WikiFact",
          "href": "?group=wikifact",
          "markdown": false
        },
        {
          "value": "Scenario introduced in this work, inspired by [Petroni et al. (2019)](https://aclanthology.org/D19-1250/), to more extensively test factual knowledge.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "bAbI",
          "href": "?group=babi_qa",
          "markdown": false
        },
        {
          "value": "The bAbI benchmark for measuring understanding and reasoning [(Weston et al., 2015)](https://arxiv.org/pdf/1502.05698.pdf).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Dyck",
          "href": "?group=dyck_language",
          "markdown": false
        },
        {
          "value": "Scenario testing hierarchical reasoning through the Dyck formal languages [(Suzgun et al., 2019)](https://aclanthology.org/W19-3905/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Numerical reasoning",
          "href": "?group=numeracy",
          "markdown": false
        },
        {
          "value": "Scenario introduced in this work to test numerical reasoning via symbolic regression.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Synthetic reasoning (abstract symbols)",
          "href": "?group=synthetic_reasoning",
          "markdown": false
        },
        {
          "value": "Synthetic reasoning tasks defined using abstract symbols based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Synthetic reasoning (natural language)",
          "href": "?group=synthetic_reasoning_natural",
          "markdown": false
        },
        {
          "value": "Synthetic reasoning tasks defined using simple natural language based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "GSM8K (Grade school math word problems)",
          "href": "?group=gsm",
          "markdown": false
        },
        {
          "value": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "MATH",
          "href": "?group=math_regular",
          "markdown": false
        },
        {
          "value": "The MATH benchmark for measuring mathematical problem solving on competition math problems [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "MATH (chain-of-thought)",
          "href": "?group=math_chain_of_thought",
          "markdown": false
        },
        {
          "value": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "APPS (Code)",
          "href": "?group=code_apps",
          "markdown": false
        },
        {
          "value": "The APPS benchmark for measuring competence on code challenges [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "HumanEval (Code)",
          "href": "?group=code_humaneval",
          "markdown": false
        },
        {
          "value": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "LegalBench",
          "href": "?group=legalbench",
          "markdown": false
        },
        {
          "value": "LegalBench is a large collaboratively constructed benchmark of legal reasoning. Five representative tasks are included here. See [(Guha et al, 2023)[https://arxiv.org/abs/2308.11462] for more details.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "LegalSupport",
          "href": "?group=legal_support",
          "markdown": false
        },
        {
          "value": "Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "LSAT",
          "href": "?group=lsat_qa",
          "markdown": false
        },
        {
          "value": "The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; [Zhong et al., 2021](https://arxiv.org/pdf/2104.06598.pdf)).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "MedQA",
          "href": "?group=med_qa",
          "markdown": false
        },
        {
          "value": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "WMT 2014",
          "href": "?group=wmt_14",
          "markdown": false
        },
        {
          "value": "WMT 2014 is a collection of machine translation datasets.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "LEXTREME",
          "href": "?group=lextreme",
          "markdown": false
        },
        {
          "value": "A Multilingual Legal Benchmark for Natural Language Understanding",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "LexGLUE",
          "href": "?group=lex_glue",
          "markdown": false
        },
        {
          "value": "A Benchmark Dataset for Legal Language Understanding in English",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "BillSum",
          "href": "?group=billsum_legal_summarization",
          "markdown": false
        },
        {
          "value": "The BillSum benchmark for legal text summarization ([Kornilova & Eidelmann, 2020](https://aclanthology.org/D19-5406/)).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "MultiLexSum",
          "href": "?group=multilexsum_legal_summarization",
          "markdown": false
        },
        {
          "value": "The MultiLexSum benchmark for legal text summarization ([Shen et al., 2022](https://arxiv.org/abs/2206.10883)).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "EurLexSum",
          "href": "?group=eurlexsum_legal_summarization",
          "markdown": false
        },
        {
          "value": "The EurLexSum benchmark for legal text summarization ([Aumiller et al., 2022](https://arxiv.org/abs/2210.13448)).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Data imputation",
          "href": "?group=entity_data_imputation",
          "markdown": false
        },
        {
          "value": "Scenario from [Mei et al. (2021)](https://ieeexplore.ieee.org/document/9458712/) that tests the ability to impute missing entities in a data table.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Entity matching",
          "href": "?group=entity_matching",
          "markdown": false
        },
        {
          "value": "Scenario from Magellan [(Konda et al., 2016)](https://dl.acm.org/doi/10.14778/3007263.3007314) that tests the ability to determine if two entities match.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Copyright (text)",
          "href": "?group=copyright_text",
          "markdown": false
        },
        {
          "value": "Scenario introduced in this work to measure copyright and memorization behavior for books, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Copyright (code)",
          "href": "?group=copyright_code",
          "markdown": false
        },
        {
          "value": "Scenario introduced in this work to measure copyright and memorization behavior for code, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Disinformation (reiteration)",
          "href": "?group=disinformation_reiteration",
          "markdown": false
        },
        {
          "value": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to reiterate disinformation content.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Disinformation (wedging)",
          "href": "?group=disinformation_wedging",
          "markdown": false
        },
        {
          "value": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to generate divisive and wedging content.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "BBQ (Bias Benchmark for Question Answering)",
          "href": "?group=bbq",
          "markdown": false
        },
        {
          "value": "The Bias Benchmark for Question Answering (BBQ) for measuring social bias in question answering in ambiguous and unambigous context [(Parrish et al., 2022)](https://aclanthology.org/2022.findings-acl.165/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "BOLD (Bias in Open-Ended Language Generation Dataset)",
          "href": "?group=bold",
          "markdown": false
        },
        {
          "value": "The Bias in Open-Ended Language Generation Dataset (BOLD) for measuring biases and toxicity in open-ended language generation [(Dhamala et al., 2021)](https://dl.acm.org/doi/10.1145/3442188.3445924).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "RealToxicityPrompts",
          "href": "?group=real_toxicity_prompts",
          "markdown": false
        },
        {
          "value": "The RealToxicityPrompts dataset for measuring toxicity in prompted model generations [(Gehman et al., 2020)](https://aclanthology.org/2020.findings-emnlp.301/).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Synthetic efficiency",
          "href": "?group=synthetic_efficiency",
          "markdown": false
        },
        {
          "value": "Scenario introduced in this work to better understand inference runtime performance of various models.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) closed book question answering",
          "href": "?group=cleva_closed_book_question_answering",
          "markdown": false
        },
        {
          "value": "Closed-book question answering task comprises three subtasks. One is for the medical domain, another for open-domain, and the last measures if a model generates truthful answers.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) summarization",
          "href": "?group=cleva_summarization",
          "markdown": false
        },
        {
          "value": "Summarize a dialogue between a customer representative and a customer.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) text classification",
          "href": "?group=cleva_text_classification",
          "markdown": false
        },
        {
          "value": "This scenario has two subtasks. Classify if an utterance is humorous and identify news topic based on its title.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) translation",
          "href": "?group=cleva_translation",
          "markdown": false
        },
        {
          "value": "Scenario for measuring the translation quality between Chinese and English.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) data to text generation",
          "href": "?group=cleva_data_to_text_generation",
          "markdown": false
        },
        {
          "value": "Generate a product description based on structured data containing various product properties.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) dialogue generation",
          "href": "?group=cleva_dialogue_generation",
          "markdown": false
        },
        {
          "value": "Task-oriented dialogue between a user and a system.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) opinion mining",
          "href": "?group=cleva_opinion_mining",
          "markdown": false
        },
        {
          "value": "Extract the target of an opinion.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) paraphrase generation",
          "href": "?group=cleva_paraphrase_generation",
          "markdown": false
        },
        {
          "value": "Generate a paraphrase of a given sentence.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) paraphrase identification",
          "href": "?group=cleva_paraphrase_identification",
          "markdown": false
        },
        {
          "value": "Identify if two sentences, from a dialogue or from the finance domain, share the same meaning.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) reading comprehension",
          "href": "?group=cleva_reading_comprehension",
          "markdown": false
        },
        {
          "value": "Answer a multiple-choice question based on a given paragraph.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) sentiment analysis",
          "href": "?group=cleva_sentiment_analysis",
          "markdown": false
        },
        {
          "value": "Chinese sentiment analysis for product reviews.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) language modeling",
          "href": "?group=cleva_language_modeling",
          "markdown": false
        },
        {
          "value": "Scenario for measuring language model performance across various domains (wikipedia and news).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) pinyin transliteration",
          "href": "?group=cleva_pinyin_transliteration",
          "markdown": false
        },
        {
          "value": "Scenario that asks the model to translate between Chinese and Pinyin.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) classical Chinese understanding",
          "href": "?group=cleva_classical_chinese_understanding",
          "markdown": false
        },
        {
          "value": "Scenario for evaluating the understanding of classical Chinese by selecting the appropriate classical Chinese translation for a given modern Chinese sentence.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) coreference resolution",
          "href": "?group=cleva_coreference_resolution",
          "markdown": false
        },
        {
          "value": "Scenario for testing models on solving coreference resolution problems (the winograd schema challenge).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) intent understanding",
          "href": "?group=cleva_intent_understanding",
          "markdown": false
        },
        {
          "value": "Tests whether the model could capture the writing intention of the authors after reading an article.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) subject knowledge",
          "href": "?group=cleva_subject_knowledge",
          "markdown": false
        },
        {
          "value": "Scenario inspired by [Petroni et al. (2019)](https://aclanthology.org/D19-1250/) to extensively test factual knowledge in Chinese. It contains 13 subjects and a general domain.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) cultural knowledge",
          "href": "?group=cleva_cultural_knowledge",
          "markdown": false
        },
        {
          "value": "Scenario for evaluating models' understanding of Chinese culture. It has a Chinese-idiom-focused subtask.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) reasoning primitive",
          "href": "?group=cleva_reasoning_primitive",
          "markdown": false
        },
        {
          "value": "Scenario focused on primitive reasoning, including dyck language continuation, variable substitution, pattern induction, and pattern matching.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) deductive reasoning",
          "href": "?group=cleva_deductive_reasoning",
          "markdown": false
        },
        {
          "value": "Scenario that gauges model's ability to reason deductive arguments. It includes a modus tollens subtask.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) inductive reasoning",
          "href": "?group=cleva_inductive_reasoning",
          "markdown": false
        },
        {
          "value": "Scenario that tests models' ability to conclude rules from demonstrations and apply them to unseen test instances.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) code synthesis",
          "href": "?group=cleva_code_synthesis",
          "markdown": false
        },
        {
          "value": "Scenario for measuring functional correctness for synthesizing programs from Chinese docstrings.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) commonsense reasoning",
          "href": "?group=cleva_commonsense_reasoning",
          "markdown": false
        },
        {
          "value": "Scenario that tests models' commonsense reasoning ability. There are two subtasks: textual entailment and commonsense question answering.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) mathematical reasoning",
          "href": "?group=cleva_mathematical_reasoning",
          "markdown": false
        },
        {
          "value": "Scenario that tests models' mathematical reasoning ability with chain-of-thought style reasoning. It contains a math word problem solving subtask.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) conceptual generalization",
          "href": "?group=cleva_conceptual_generalization",
          "markdown": false
        },
        {
          "value": "Scenario that assesses whether models could generalize physical relations to a synthetic grid world.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) toxicity detection",
          "href": "?group=cleva_toxicity_detection",
          "markdown": false
        },
        {
          "value": "Ask models about the offensiveness of the given text.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) bias",
          "href": "?group=cleva_bias",
          "markdown": false
        },
        {
          "value": "Scenario that gauges bias of four demographic categories in dialogues, including race, gender, region, and occupation.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) copyright",
          "href": "?group=cleva_copyright",
          "markdown": false
        },
        {
          "value": "Scenario that measures copyright and memorization behavior for Chinese books and code, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) fact checking",
          "href": "?group=cleva_fact_checking",
          "markdown": false
        },
        {
          "value": "Scenario that lets models identify whether the given fact is true to test their factuality.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) instruction following",
          "href": "?group=cleva_instruction_following",
          "markdown": false
        },
        {
          "value": "Scenario that examines whether models could follow human instructions, mainly uncommon ones. It contains two subtasks: 'redefine' and 'pattern_matching_suppression'.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "CLEVA (Chinese) mathematical calculation",
          "href": "?group=cleva_mathematical_calculation",
          "markdown": false
        },
        {
          "value": "Scenario that evaluates the calculation ability of models. It has four subtasks: three-digit addition, three-digit subtraction, two-digit multiplication, and significant figures.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Data-to-text generation",
          "href": "?group=data_to_text_generation",
          "markdown": false
        },
        {
          "value": "Currently, we prioritize user-facing tasks in our core scenarios, but don't implement data-to-text generation. Could be implemented via WebNLG, E2E, ToTTo, etc.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Fact verification",
          "href": "?group=fact_verification",
          "markdown": false
        },
        {
          "value": "Currently, we prioritize user-facing tasks in our core scenarios, but don't implement fact verification. Could be implemented via FEVER.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Copywriting",
          "href": "?group=copywriting",
          "markdown": false
        },
        {
          "value": "Currently, we prioritize user-facing tasks in our core scenarios, but don't implement tasks that have not been historically studied in the NLP research community like (ad) copywriting.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Story generation",
          "href": "?group=story_generation",
          "markdown": false
        },
        {
          "value": "Currently, we prioritize user-facing tasks in our core scenarios, but don't implement more creative and interactive tasks like story generation.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Biomedical scenarios",
          "href": "?group=biomedical_scenarios",
          "markdown": false
        },
        {
          "value": "Currently, we implement scenarios from common domains in NLP research, neglecting various domains where language technologies could provide significant value.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Clinical scenarios",
          "href": "?group=clinical_scenarios",
          "markdown": false
        },
        {
          "value": "Currently, we implement scenarios from common domains in NLP research, neglecting various domains where language technologies could provide significant value.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Financial scenarios",
          "href": "?group=financial_scenarios",
          "markdown": false
        },
        {
          "value": "Currently, we implement scenarios from common domains in NLP research, neglecting various domains where language technologies could provide significant value.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Customer services scenarios",
          "href": "?group=customer_service_scenarios",
          "markdown": false
        },
        {
          "value": "Currently, we implement scenarios from common domains in NLP research, neglecting various domains where language technologies could provide significant value.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Educational scenarios",
          "href": "?group=educational_scenarios",
          "markdown": false
        },
        {
          "value": "Currently, we implement scenarios from common domains in NLP research, neglecting various domains where language technologies could provide significant value.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Very recent scenarios",
          "href": "?group=very_recent_scenarios",
          "markdown": false
        },
        {
          "value": "Currently, we implement scenarios using standard NLP datasets. However, to test temporal generalization as the world and language change, we should implement scenarios with very recent data (e.g., current world events) like StreamingQA.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Scenarios involving historic data",
          "href": "?group=historical_scenarios",
          "markdown": false
        },
        {
          "value": "Currently, we implement scenarios using standard NLP datasets, which predominantly are from post-Internet and contemporary society. However, to test temporal generalization for using models in the digital humanities for historic data, we should implement scenarios with significantly older data (e.g., text from 1800s).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Scenarios involving non-native speakers",
          "href": "?group=not_native_English_speaker",
          "markdown": false
        },
        {
          "value": "Currently, we implement scenarios of an unknown composition of native and non-native English speakers. We should implement scenarios to ensure coverage of language from non-native English speakers.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Scenarios involving data from marginalized demographics in non-US English-speaking regions",
          "href": "?group=non_US_demographics",
          "markdown": false
        },
        {
          "value": "Currently, we ensure some coverage of language based on US-centric demographic groups, including marginalized groups. We should implement scenarios to ensure coverage of other socially-relevant groups beyond US demographics (e.g., caste in India).",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Scenarios with user-facing tasks on English dialects",
          "href": "?group=user_facing_tasks_english_dialects",
          "markdown": false
        },
        {
          "value": "Currently, evaluate performance on English dialects via language modeling (e.g., TwitterAAE, ICE), but it would be good to implement user-facing tasks for these dialects.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "DecodingTrust - AdvGLUE++",
          "href": "?group=decodingtrust_adv_robustness",
          "markdown": false
        },
        {
          "value": "Adversarial perturbations of the GLUE dataset generated against open-source LLMs including Alpaca, Vicuna, and Stable-Vicuna",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "DecodingTrust - Adversarial Demonstrations",
          "href": "?group=decodingtrust_adv_demonstration",
          "markdown": false
        },
        {
          "value": "Robustness analysis of LM generations when facing adversarial demonstrations",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "DecodingTrust - OoD Robustness",
          "href": "?group=decodingtrust_ood_robustness",
          "markdown": false
        },
        {
          "value": "Style perturbations of GLUE datasets (OoD styles) and out-of-scope OoD knowledge evaluations",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "DecodingTrust - Fairness",
          "href": "?group=decodingtrust_fairness",
          "markdown": false
        },
        {
          "value": "Fairness analysis of LLMs",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "DecodingTrust - Privacy",
          "href": "?group=decodingtrust_privacy",
          "markdown": false
        },
        {
          "value": "Evaluation of the privacy understanding and privacy preserving properties of LLMs",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "DecodingTrust - Ethics",
          "href": "?group=decodingtrust_machine_ethics",
          "markdown": false
        },
        {
          "value": "Evaluation of the understanding of ethical behaviors of LLMs",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "DecodingTrust - Toxicity",
          "href": "?group=decodingtrust_toxicity_prompts",
          "markdown": false
        },
        {
          "value": "Evaluation of the privacy understanding and privacy preserving properties of LLMs",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "DecodingTrust - Stereotype Bias",
          "href": "?group=decodingtrust_stereotype_bias",
          "markdown": false
        },
        {
          "value": "Manually crafted stereotype user prompts from DecodingTrust",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ],
      [
        {
          "value": "Thai Exam",
          "href": "?group=thai_exam",
          "markdown": false
        },
        {
          "value": "A benchmark comprising Thai multiple-choice examinations.",
          "markdown": true
        },
        {
          "value": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 0,
          "markdown": false
        }
      ]
    ],
    "links": []
  }
]